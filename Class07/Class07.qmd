---
title: "Class 7: Machine Learning 1"
author: "Carly Chang (PID A16843962)"
format: html
---

Today, we will explore unsupervised machine learning methods including clustering and dimensionality reduction methods.

Let's start by making up some data (where we know there are clear groups that we can use to test out different clustering methods).

We can use the `rnorm()` function to help us here, which randomly generates data with normal distrubution of mean and sd.
```{r}
hist(rnorm(n=30000, mean=3)) #hist() makes a histogram
```


Make data `z` with two "clusters" 

```{r}
x <- c(rnorm(30, mean=-3),
  rnorm(30, mean=+3))
z <- cbind(x=x, y=rev(x)) #two columns of x and y. rev(x) reverses the order of x
head(z)
plot(z)
```

## K-means clustering

The main function in "base" R for K-means clustering is called `kmeans()`. This will group an input data into input number of clusters that each center around its mean points.

```{r}
k <- kmeans(z, centers=2) #Makes 2 clusters, with sizes of matrix z (30, 30). Clustering vector: each point is assigned to either cluster 1 or 2.
k
```

```{r}
attributes(k) #gives components of k
```

> Q. How many points lie in each cluster?

```{r}
k$size
```


> Q. What component of our results tells us about the cluster membership (ie. which point lies in which cluster)?

```{r}
k$cluster
```


> Q. Center of each cluster?

```{r}
k$center
```


> Q. Put this result info together and make a little "base R" plot of our clustsering result. Also add the cluster center points to this plot.

```{r}
plot(z, col=c("blue","red")) #alternates blue and red points across z
```


You can also color by number (1=black):
```{r}
plot(z, col=c(1,2))
```


Color by membership/cluster:
```{r}
plot(z, col=k$cluster)
points(k$centers, col="blue", pch=16) #add points at center values in blue as solid filled circle (pch)
```


> Q. Run k-means on our input `z` and define 4 clusters making the same results visualization plot as above (plot of z colored by cluster membership)

```{r}
k4 <- kmeans(z, centers=4)
k4
plot(z, col=k$cluster)
points(k$centers, col="blue", pch=16)
```

Better plot clustering will have a smaller tot.withinss value
```{r}
k$tot.withinss #better clustering
k4$tot.withinss
```

You can also plot a scree plot (x=number of clusters, y=tot.withinss) and "elbow" in the plot will determine the number of clusters that you should have.


## Hierarchical Clustering

The main function in base R for this is called `hclust()`. It will take as input a distance matrix (you cannot just give your "raw" data as input - you have to first calculate a distance matrix from your data).

```{r}
d <- dist(z) #calculates distance between the rows of a data matrix
hc <- hclust(d)
hc
```

Plot hclust, which will produce a hierarchical tree of the input values. All lower numbers <30 are on the left side and higher numbers 31-60 are on the right side. Each point starts as it's own cluster and the closest points are grouped together until there is one cluster left as the highest branch in the hierarchy. Higher branches = points farther apart.
```{r}
plot(hc)
abline(h=10, col="red") #draws a line at height 10 (where we will cut)
```

Once I inspect the "tree", I can "cut" the tree at a certain height to yield my groupings or clusters. The function to do this is called `cutree()`.

```{r}
grps <- cutree(hc, h=10) #returns the cluster assignments below the cut. There are 2 clusters below h=10
grps
```

```{r}
plot(z, col=grps)
```

There are 4 methods to determine distance between clusters in hclust() - use trial and error.

## Hands on with Principal Component Analysis (PCA)

Let's examine some silly 17-dimensional data detailing food consumption in the UK (England, Scotland, Wales, and N. Ireland). Are these countries eating habits different or similar to one another?

### Data import
```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url) #returns 5 columns (first being row names), but we only want 4 columns
x
```

> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this question?

```{r}
nrow(x)
ncol(x)
dim(x)
```

> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

Set rownames() to the first column and removes the first column with the -1 colum index:
```{r}
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```

Running this code multiple times will continuously remove the first column. 

The code below will keep the first column (foods) as the row name, without deleting any other columns after multiple runs.
```{r}
x <- read.csv(url, row.names=1)
head(x)
```


```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

> Q3: Changing what optional argument in the above barplot() function results in the following plot?

Setting beside = FALSE puts the bar plots on top of each other.
```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

> Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

The label is the y-axis in its corresponding row and the x-axis in its corresponding column. For example, England is on y-axis across the first row, Wales is on x-axis on the second column.
Points on the diagonal means that the two countries are similar. Points above the diagonal means the value is higher for the y-axis; points below the diagonal means the value is higher for the x-axis.
```{r}
pairs(x, col=rainbow(10), pch=16)
```
> Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

N. Ireland is the most different from the other countries, as we can see in the vertical column and horizontal row corresponding to N. Ireland. It contains the most points off the diagonal.

Looking at these types of "pairwise plots" can be helpful but it does not scale well and kind of sucks! There must be a better way....

### PCA to the rescue!

Principal component analysis (PCA) is a well established "multivariate statistical technique" used to reduce the dimensionality of a complex data set to a more manageable number (typically 2D or 3D). In our example here, we have 17 dimensional data for 4 countries. We can thus ‘imagine’ plotting the 4 coordinates representing the 4 countries in 17 dimensional space. If there is any correlation between the observations (the countries), this will be observed in the 17 dimensional space by the correlated points being clustered close together.

The main function for PCA in base R is called `prcomp()`. This function wants the transpose of our input data - ie. the important foods in as columns and the countries as rows

```{r}
pca <- prcomp(t(x)) #t() flips rows (countries) and columns (foods)
summary(pca) #gives properties of each axis (PC1, PC2,...)
```

Proportion of variance tells you percentage of data that is captured by that particular axis. PC1 has the highest variance.
Cumulative proportion gives proportion of data that is captured by all the axis so far (ie. 96.5% captured by 2 axes).


Let's see what is in our PCA result object `pca`:

```{r}
attributes(pca)
```

The `pca$x` result object is where we will focus first as this details how the countries are related to each other in terms of our new "axis (aka "PCs", "eigenvectors", etc.)

```{r}
head(pca$x)
```
> Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

```{r}
plot(pca$x[,1], pca$x[,2], col=c("orange","red","blue","green"),
     xlab = "PC1", ylab="PC2")
text(pca$x[,1], pca$x[,2], colnames(x)) #adds text as the country names
```

> Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
plot(pca$x[,1], pca$x[,2], col=c("orange","red","blue","green"),
     xlab = "PC1", ylab="PC2")
text(pca$x[,1], pca$x[,2], colnames(x),col=c("orange","red","blue","green"))
```


We can look at the so-called PC "loadings" result object to see how the original foods contribute to our new PCs (ie. how the original variables contribute to our new better PC variables). A positive loading is a positive correlation, while a negative loading is a negative correlation between food and a particular PC.

```{r}
pca$rotation[,1] #PC1
```

Plot a bar plot representing PC1:
```{r}
par(mar=c(10, 3, 0.35, 0)) #plot margins (botoom, left, top, right)
barplot(pca$rotation[,1], las=2) #las=2 makes axis labels vertical
```


> Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?

```{r}
pca$rotation[,2] #PC2
barplot( pca$rotation[,2], las=2 )
```

Soft drinks (0.555) and fresh potatoes (-0.715) predominate. The "PC2" column tells us the contribution of each food category along the PC2 axis, which could possibly

### Using ggplot for these figures

```{r}
library(ggplot2)

df <- as.data.frame(pca$x) #convert PCA data into data frame
df_lab <- tibble::rownames_to_column(df, "Country") #add column called "Country"
# Our first basic plot
ggplot(df_lab) + 
  aes(PC1, PC2, col=Country) + 
  geom_point()
```

To plot our loadings plot with ggplot, we will convert it to a data frame and add row names as a new column called "Food":
```{r}
ld <- as.data.frame(pca$rotation)
ld_lab <- tibble::rownames_to_column(ld, "Food")

ggplot(ld_lab) +
  aes(PC1, reorder(Food, PC1), fill=PC1) +
  geom_col() + 
  xlab("PC1 Loadings/Contributions") +
  ylab("Food Group") +
  scale_fill_gradient2(low="purple", mid="gray", high="darkgreen", guide=NULL) + #colors by value and removes legend
  theme_bw()
```


### Biplots
Another way to visualize PCA information is in a biplot. The data is organized in a central group of foods around the middle of each PC, with some on the periphery. Points closer to each other are more similar and longer arrows pointing towards/away the PC axis mean the variable contributes more to the PC.

```{r}
biplot(pca)
```

# PCA of RNA-seq data

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

> Q10: How many genes and samples are in this data set?

```{r}
nrow(rna.data) #genes
ncol(rna.data) #samples
```


Let's make a PCA and plot the results:
```{r}
pca <- prcomp(t(rna.data), scale=TRUE)
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2")
```

```{r}
summary(pca)
```

Quick plot of proportion of variance for each PC:
```{r}
plot(pca, main="Quick scree plot")
```


Let's make our own scree plot:
```{r}
pca.var <- pca$sdev^2 #variance per PC
pca.var.per <- round(pca.var/sum(pca.var)*100, 1) # Percent variance is often more informative to look at 

barplot(pca.var.per, main="Scree plot",
      names.arg = paste0("PC", 1:10),
      xlab="Principal Component", ylab="Percent Variation")
```

We can see from the summary and the Scree plots that PC1 is where all the action is (92.6%).

Making our PCA plot more attractive and useful:
```{r}
## A vector of colors for wt and ko samples
colvec <- colnames(rna.data)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"

plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
     xlab=paste0("PC1 (", pca.var.per[1], "%)"),
     ylab=paste0("PC2 (", pca.var.per[2], "%)"))

text(pca$x[,1], pca$x[,2], labels = colnames(rna.data), pos=c(rep(4,5), rep(2,5)))
```

Using ggplot to plot our RNA-seq data. Again, we must convert PCA to a data frame first:
```{r}
library(ggplot2)

df <- as.data.frame(pca$x)

#Basic plot
ggplot(df) + 
  aes(PC1, PC2) + 
  geom_point()
```

```{r}
df$samples <- colnames(rna.data) #column names: wt1-5 and ko1-5
df$condition <- substr(colnames(rna.data),1,2) #characters 1-2 of the colnames = "wt" and "ko"
ggplot(df) + 
  aes(PC1, PC2, label=samples, col=condition) + 
  geom_label(show.legend = FALSE)
```


